{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Basic features and readability scores\n",
    "\n",
    "### Introduction to NLP feature engineering\n",
    "\n",
    "## One-hot encoding\n",
    "\n",
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
    "\n",
    "# Print the new features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Print first five rows of df1\n",
    "print(df1.head(5))\n",
    "\n",
    "\n",
    "### Basic feature extraction\n",
    "\n",
    "## Character count of Russian tweets\n",
    "\n",
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())\n",
    "\n",
    "\n",
    "## Word count of TED talks\n",
    "\n",
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())\n",
    "\n",
    "\n",
    "## Hashtags and mentions in Russian tweets\n",
    "\n",
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()\n",
    "\n",
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### Readability tests\n",
    "\n",
    "## Readability of 'The Myth of Sisyphus'\n",
    "\n",
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))\n",
    "\n",
    "\n",
    "## Readability of various publications\n",
    "\n",
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Text preprocessing, POS tagging and NER\n",
    "\n",
    "### Tokenization and Lemmatization\n",
    "\n",
    "## Tokenizing the Gettysburg Address\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))\n",
    "\n",
    "\n",
    "\n",
    "### Text cleaning\n",
    "\n",
    "## Cleaning a blog post\n",
    "\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))\n",
    "\n",
    "\n",
    "## Cleaning TED talks in a dataframe\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])\n",
    "\n",
    "\n",
    "\n",
    "### Part-of-speech tagging\n",
    "\n",
    "## POS tagging in Lord of the Flies\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)\n",
    "\n",
    "\n",
    "## Counting nouns in a piece of text\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n",
    "\n",
    "\n",
    "## Noun usage in fake news\n",
    "\n",
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "\n",
    "\n",
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))\n",
    "\n",
    "\n",
    "\n",
    "### Named entity recognition\n",
    "\n",
    "## Named entities in a sentence\n",
    "\n",
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "\n",
    "## Identifying people mentioned in a news article\n",
    "\n",
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(tc)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(tc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ N-Gram models\n",
    "\n",
    "### Building a bag of words model\n",
    "\n",
    "## BoW model for movie taglines\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)\n",
    "\n",
    "\n",
    "## Analyzing dimensionality and preprocessing\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
    "\n",
    "# Print the shape of bow_lem_matrix\n",
    "print(bow_lem_matrix.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Mapping feature indices with feature names\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)\n",
    "\n",
    "\n",
    "\n",
    "### Building a BoW Naive Bayes classifier\n",
    "\n",
    "## BoW vectors for movie reviews\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)\n",
    "\n",
    "\n",
    "## Predicting the sentiment of a movie review\n",
    "\n",
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
    "\n",
    "\n",
    "\n",
    "### Building n-gram models\n",
    "\n",
    "## n-gram models for movie tag lines\n",
    "\n",
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))\n",
    "\n",
    "\n",
    "## Higher order n-grams for sentiment analysis\n",
    "\n",
    "# Define an instance of MultinomialNB \n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier \n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy \n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
    "\n",
    "\n",
    "## Comparing performance of n-gram models\n",
    "\n",
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))\n",
    "\n",
    "# The program took around 0.2 seconds in the case of the unigram model and more than 10 times longer for the \n",
    "# higher order n-gram model. The unigram model had over 12,000 features whereas the n-gram model for upto n=3 had \n",
    "# over 178,000! Despite taking higher computation time and generating more features, the classifier only performs \n",
    "# marginally better in the latter case, producing an accuracy of 77% in comparison to the 75% for the unigram model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### TF-IDF and similarity scores\n",
    "\n",
    "### Building tf-idf document vectors\n",
    "\n",
    "## tf-idf vectors for TED talks\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "\n",
    "### Cosine similarity\n",
    "\n",
    "## Computing dot product\n",
    "\n",
    "# Initialize numpy vectors\n",
    "A = np.array([1,3])\n",
    "B = np.array([-2,2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)\n",
    "\n",
    "\n",
    "## Cosine similarity matrix of a corpus\n",
    "\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)\n",
    "\n",
    "\n",
    "\n",
    "### Building a plot line based recommender\n",
    "\n",
    "## Comparing linear_kernel and cosine_similarity\n",
    "\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))\n",
    "# Time taken: 0.398421049118042 seconds\n",
    "\n",
    "\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))\n",
    "# Time taken: 0.37362122535705566 seconds\n",
    "\n",
    "\n",
    "\n",
    "## Plot recommendation engine\n",
    "\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))\n",
    "\n",
    "\n",
    "## The recommender function\n",
    "\n",
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]\n",
    "\n",
    "\n",
    "## TED talk recommender\n",
    "\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Beyond n-grams: word embeddings\n",
    "\n",
    "## Generating word vectors\n",
    "\n",
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "  for token2 in doc:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))\n",
    "    \n",
    "\n",
    "## Computing similarity of Pink Floyd songs\n",
    "\n",
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))\n",
    "# 0.6006234924640204\n",
    "# 0.9135920924498578\n",
    "\n",
    "# Notice that 'Mother' and 'Hey You' have a similarity score of 0.9 whereas 'Mother' and \n",
    "# 'High Hopes' has a score of only 0.6. This is probably because 'Mother' and 'Hey You' \n",
    "# were both songs from the same album 'The Wall' and were penned by Roger Waters. On the \n",
    "# other hand, 'High Hopes' was a part of the album 'Division Bell' with lyrics by David \n",
    "# Gilmour and his wife, Penny Samson. Treat yourself by listening to these songs. They're \n",
    "# some of the best!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
